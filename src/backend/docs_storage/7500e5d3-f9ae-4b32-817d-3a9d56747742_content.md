# LLM2Vec Documentation


## Overview

**LLM2Vec** is a powerful framework developed by McGill NLP for converting Large Language Models (LLMs) into effective text encoders. This project enables bidirectional attention mechanisms and specialized training techniques to transform decoder-only language models into high-quality sentence and word-level embeddings.


## Key Features

- **Bidirectional Attention**: Convert unidirectional LLMs to bidirectional encoders
- **Multiple Training Strategies**: Support for MNTP (Masked Next Token Prediction), SimCSE, and supervised training
- **Word-Level Tasks**: Fine-tuning capabilities for token classification tasks like POS tagging and NER
- **Model Support**: Compatible with popular models including Llama-2, Llama-3, and Mistral
- **Efficient Training**: LoRA-based parameter-efficient fine-tuning
- **Flash Attention**: Optimized attention mechanisms for better performance


## Installation

To get started with LLM2Vec, clone the repository and install the required dependencies:

```bash
git clone https://github.com/McGill-NLP/llm2vec.git
cd llm2vec
pip install -r requirements.txt
```


## Quick Start

### Basic Usage

Here's a simple example of how to use LLM2Vec for text encoding:

```python
from llm2vec import LLM2Vec

# Load a pre-trained LLM2Vec model
model = LLM2Vec.from_pretrained("McGill-NLP/LLM2Vec-Llama-2-7b-chat-hf-mntp")

# Encode text
texts = ["Hello world", "How are you?"]
embeddings = model.encode(texts)
```


## Model Architecture

### Bidirectional Attention

LLM2Vec transforms decoder-only models by enabling **bidirectional attention**, allowing tokens to attend to both past and future context. This is crucial for creating effective text encoders.

### Training Stages

The framework supports a **three-stage training approach**:

1. **MNTP (Masked Next Token Prediction)**: Enables bidirectional attention
2. **SimCSE**: Unsupervised contrastive learning for sentence embeddings  
3. **Supervised Fine-tuning**: Task-specific optimization


## Training Configuration

### MNTP Training

Configure MNTP training using JSON configuration files:

```json
{
    "model_name_or_path": "meta-llama/Llama-2-7b-chat-hf",
    "dataset_name": "wikitext",
    "dataset_config_name": "wikitext-103-raw-v1",
    "max_seq_length": 512,
    "mask_token_type": "blank",
    "mlm_probability": 0.2,
    "lora_r": 16,
    "torch_dtype": "bfloat16",
    "attn_implementation": "flash_attention_2"
}
```

### SimCSE Training

For contrastive learning with SimCSE:

```json
{
    "model_name_or_path": "meta-llama/Llama-2-7b-chat-hf",
    "peft_model_name_or_path": "McGill-NLP/LLM2Vec-Llama-2-7b-chat-hf-mntp",
    "simcse_dropout": 0.3,
    "bidirectional": true,
    "pooling_mode": "mean",
    "dataset_name": "Wiki1M",
    "learning_rate": 3e-5,
    "loss_scale": 20
}
```


## Word-Level Tasks

### Token Classification

LLM2Vec supports word-level tasks such as **Part-of-Speech (POS) tagging** and **Named Entity Recognition (NER)**:

```json
{
    "model_name_or_path": "McGill-NLP/LLM2Vec-Llama-2-7b-chat-hf-mntp",
    "model_class": "custom",
    "bidirectional": true,
    "classifier_dropout": 0.1,
    "merge_subwords": true,
    "retroactive_labels": "next_token",
    "dataset_name": "conll2003",
    "task": "pos_tags"
}
```

### Key Parameters

- **`bidirectional`**: Enable bidirectional attention
- **`merge_subwords`**: Handle subword tokenization
- **`retroactive_labels`**: Label assignment strategy (`same_token` or `next_token`)
- **`classifier_dropout`**: Dropout rate for the classification head


## Supported Models

### Pre-trained Models Available

- **Llama-2 7B Chat**: `McGill-NLP/LLM2Vec-Llama-2-7b-chat-hf-mntp`
- **Llama-3 8B Instruct**: `McGill-NLP/LLM2Vec-Meta-Llama-3-8B-Instruct-mntp`
- **Mistral 7B Instruct**: `McGill-NLP/LLM2Vec-Mistral-7B-Instruct-v2-mntp`

### Model Variants

Each model comes in different training stages:
- **Base + MNTP**: Bidirectional attention enabled
- **MNTP + SimCSE**: Additional contrastive learning
- **MNTP + Supervised**: Task-specific fine-tuning


## Configuration Files

### Training Configurations

The repository includes comprehensive configuration files organized by training type:

- **`train_configs/mntp/`**: MNTP training configurations
- **`train_configs/simcse/`**: SimCSE training configurations  
- **`train_configs/supervised/`**: Supervised training configurations
- **`train_configs/word-task/`**: Word-level task configurations

### Testing Configurations

- **`test_configs/word-task/`**: Evaluation configurations for token classification
- **`test_configs/mteb/`**: MTEB benchmark evaluation settings


## Advanced Features

### Flash Attention Support

All configurations support **Flash Attention 2** for improved memory efficiency:

```json
{
    "attn_implementation": "flash_attention_2",
    "torch_dtype": "bfloat16"
}
```

### LoRA Fine-tuning

Parameter-efficient training using **LoRA (Low-Rank Adaptation)**:

```json
{
    "lora_r": 16,
    "gradient_checkpointing": true
}
```

### Gradient Checkpointing

Memory optimization for large models:

```json
{
    "gradient_checkpointing": true,
    "per_device_train_batch_size": 32,
    "gradient_accumulation_steps": 1
}
```


## Training Commands

### MNTP Training

```bash
python train_mntp.py --config train_configs/mntp/Llama2.json
```

### SimCSE Training

```bash
python train_simcse.py --config train_configs/simcse/Llama2.json
```

### Word-Level Task Training

```bash
python train_word_task.py --config train_configs/word-task/Llama2-bi-mntp.json
```


## Evaluation

### Running Evaluations

```bash
python test_word_task.py --config test_configs/word-task/Llama2-bi-mntp-simcse.json
```

### Supported Datasets

- **CoNLL-2003**: For NER and POS tagging
- **WikiText-103**: For MNTP training
- **Wiki1M**: For SimCSE training
- **E5 Dataset**: For supervised training


## Best Practices

### Memory Optimization

1. **Use Flash Attention 2** for better memory efficiency
2. **Enable gradient checkpointing** for large models
3. **Adjust batch size** based on available GPU memory
4. **Use bfloat16** precision to reduce memory usage

### Training Tips

1. **Start with MNTP** to enable bidirectional attention
2. **Apply SimCSE** for better sentence-level representations
3. **Fine-tune on supervised data** for specific tasks
4. **Use appropriate learning rates** (3e-5 for SimCSE, 2e-4 for supervised)


## Troubleshooting

### Common Issues

1. **Out of Memory**: Reduce batch size or enable gradient checkpointing
2. **Slow Training**: Ensure Flash Attention 2 is properly installed
3. **Poor Performance**: Verify bidirectional attention is enabled
4. **Configuration Errors**: Check JSON syntax and required parameters


## Contributing

We welcome contributions to LLM2Vec! Please:

1. Fork the repository
2. Create a feature branch
3. Submit a pull request with detailed description
4. Ensure all tests pass


## Citation

If you use LLM2Vec in your research, please cite our paper:

```bibtex
@article{llm2vec2024,
  title={LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders},
  author={McGill NLP Team},
  journal={arXiv preprint},
  year={2024}
}
```


## Contact

For questions, bug reports, or contributions:

- **Email**: Contact McGill NLP team members
- **GitHub Issues**: Open an issue in the repository
- **Twitter**: Follow [@McGill_NLP](https://twitter.com/McGill_NLP)


## License

This project is licensed under the MIT License. See the LICENSE file for details.