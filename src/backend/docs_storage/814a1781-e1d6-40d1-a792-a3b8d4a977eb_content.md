# LLM2Vec Documentation


## Overview

**LLM2Vec** is a powerful framework developed by McGill NLP for converting Large Language Models (LLMs) into effective text encoders. This project enables bidirectional attention mechanisms and provides comprehensive training pipelines for creating high-quality sentence embeddings from popular LLMs like Llama-2, Llama-3, and Mistral.


## Key Features

- **Bidirectional Attention**: Convert unidirectional LLMs to bidirectional encoders
- **Multiple Training Stages**: Support for MNTP (Masked Next Token Prediction), SimCSE, and supervised training
- **Pre-trained Models**: Ready-to-use models available on Hugging Face
- **Word-level Tasks**: Support for POS tagging and other token classification tasks
- **Flexible Configuration**: JSON-based configuration system for easy experimentation
- **Multiple Model Support**: Compatible with Llama-2, Llama-3, and Mistral architectures


## Quick Start


### Installation

```bash
git clone https://github.com/McGill-NLP/llm2vec.git
cd llm2vec
pip install -r requirements.txt
```


### Basic Usage

```python
from llm2vec import LLM2Vec

# Load a pre-trained LLM2Vec model
model = LLM2Vec.from_pretrained("McGill-NLP/LLM2Vec-Llama-2-7b-chat-hf-mntp")

# Encode sentences
sentences = ["Hello world", "How are you?"]
embeddings = model.encode(sentences)
```


## Model Architecture


### Bidirectional Attention

LLM2Vec transforms unidirectional language models into bidirectional encoders by:

- **Enabling bidirectional attention** in transformer layers
- **Modifying attention masks** to allow tokens to attend to future positions
- **Preserving pre-trained weights** while adapting the attention mechanism


### Training Pipeline

The framework follows a **three-stage training approach**:

1. **MNTP (Masked Next Token Prediction)**: Adapts the model to bidirectional context
2. **SimCSE**: Unsupervised contrastive learning for sentence representations
3. **Supervised Fine-tuning**: Task-specific training on labeled data


## Available Models

All models are available on Hugging Face under the McGill-NLP organization:


### Llama-2 Based Models

- `McGill-NLP/LLM2Vec-Llama-2-7b-chat-hf-mntp`
- `McGill-NLP/LLM2Vec-Llama-2-7b-chat-hf-mntp-unsup-simcse`


### Llama-3 Based Models

- `McGill-NLP/LLM2Vec-Meta-Llama-3-8B-Instruct-mntp`


### Mistral Based Models

- `McGill-NLP/LLM2Vec-Mistral-7B-Instruct-v2-mntp`
- `McGill-NLP/LLM2Vec-Mistral-7B-Instruct-v2-mntp-unsup-simcse`


## Training Your Own Models


### Stage 1: MNTP Training

Train a model with Masked Next Token Prediction:

```bash
python train_mntp.py --config train_configs/mntp/Llama2.json
```

**Key MNTP Configuration Parameters:**

```json
{
    "model_name_or_path": "meta-llama/Llama-2-7b-chat-hf",
    "dataset_name": "wikitext",
    "dataset_config_name": "wikitext-103-raw-v1",
    "mask_token_type": "blank",
    "mlm_probability": 0.2,
    "max_seq_length": 512,
    "lora_r": 16,
    "torch_dtype": "bfloat16",
    "attn_implementation": "flash_attention_2"
}
```


### Stage 2: SimCSE Training

Apply contrastive learning for better sentence representations:

```bash
python train_simcse.py --config train_configs/simcse/Llama2.json
```

**Key SimCSE Configuration Parameters:**

```json
{
    "peft_model_name_or_path": "McGill-NLP/LLM2Vec-Llama-2-7b-chat-hf-mntp",
    "simcse_dropout": 0.3,
    "bidirectional": true,
    "pooling_mode": "mean",
    "dataset_name": "Wiki1M",
    "loss_scale": 20,
    "max_seq_length": 128
}
```


### Stage 3: Supervised Training

Fine-tune on labeled data for specific tasks:

```bash
python train_supervised.py --config train_configs/supervised/Llama2.json
```

**Key Supervised Training Parameters:**

```json
{
    "dataset_name": "E5",
    "dataset_file_path": "cache/echo-data",
    "learning_rate": 2e-4,
    "num_train_epochs": 3,
    "warmup_steps": 300,
    "max_seq_length": 512
}
```


## Word-Level Tasks


### POS Tagging

LLM2Vec supports token classification tasks like Part-of-Speech tagging:

```bash
# Training
python train_word_task.py --config train_configs/word-task/Llama2-bi-mntp.json

# Testing
python test_word_task.py --config test_configs/word-task/Llama2-bi-mntp.json
```

**Key Word Task Parameters:**

- `bidirectional`: Enable bidirectional attention
- `merge_subwords`: Handle subword tokenization
- `retroactive_labels`: Label assignment strategy ("same_token" or "next_token")
- `classifier_dropout`: Dropout rate for classification head


## Configuration System


### Configuration Files

The project uses **JSON configuration files** organized by training stage and model:

```
train_configs/
├── mntp/           # MNTP training configs
├── simcse/         # SimCSE training configs
├── supervised/     # Supervised training configs
└── word-task/      # Word-level task configs

test_configs/
└── word-task/      # Testing configurations
```


### Common Parameters

**Model Configuration:**
- `model_name_or_path`: Base model identifier
- `peft_addr`: Path to PEFT adapter
- `torch_dtype`: Precision (bfloat16 recommended)
- `attn_implementation`: Attention implementation (flash_attention_2)

**Training Configuration:**
- `learning_rate`: Learning rate
- `per_device_train_batch_size`: Batch size per device
- `gradient_accumulation_steps`: Gradient accumulation
- `max_seq_length`: Maximum sequence length
- `output_dir`: Output directory for checkpoints


## Performance Optimization


### Memory Efficiency

- **LoRA (Low-Rank Adaptation)**: Reduces memory usage with `lora_r` parameter
- **Gradient Checkpointing**: Enabled with `gradient_checkpointing: true`
- **Flash Attention**: Use `attn_implementation: "flash_attention_2"`
- **Mixed Precision**: Set `torch_dtype: "bfloat16"`


### Training Optimization

```json
{
    "gradient_checkpointing": true,
    "torch_dtype": "bfloat16",
    "attn_implementation": "flash_attention_2",
    "lora_r": 16,
    "gradient_accumulation_steps": 1
}
```


## Evaluation and Testing


### MTEB Evaluation

Evaluate on the Massive Text Embedding Benchmark:

```bash
python evaluate_mteb.py --model_name_or_path McGill-NLP/LLM2Vec-Llama-2-7b-chat-hf-mntp
```


### Word Task Evaluation

Test on token classification tasks:

```bash
python test_word_task.py --config test_configs/word-task/Llama2-bi-mntp-simcse.json
```


## Advanced Usage


### Custom Datasets

To use custom datasets, modify the configuration:

```json
{
    "dataset_name": "custom",
    "dataset_file_path": "path/to/your/dataset.txt",
    "data_collator_type": "default"
}
```


### Multi-GPU Training

The framework supports distributed training:

```bash
torchrun --nproc_per_node=4 train_mntp.py --config train_configs/mntp/Llama2.json
```


### Pooling Strategies

Different pooling modes are available:

- `mean`: Mean pooling over token embeddings
- `cls`: Use CLS token representation
- `max`: Max pooling over sequence


## Troubleshooting


### Common Issues

**Memory Issues:**
- Reduce `per_device_train_batch_size`
- Increase `gradient_accumulation_steps`
- Enable `gradient_checkpointing`
- Use smaller `lora_r` values

**Training Instability:**
- Adjust `learning_rate`
- Modify `warmup_steps`
- Check `loss_scale` for SimCSE training

**Model Loading Errors:**
- Verify model paths in configuration
- Ensure PEFT adapters are correctly specified
- Check Hugging Face model availability


## Contributing

We welcome contributions! Please:

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests if applicable
5. Submit a pull request


## Citation

If you use LLM2Vec in your research, please cite:

```bibtex
@article{llm2vec2024,
  title={LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders},
  author={McGill NLP Team},
  journal={arXiv preprint},
  year={2024}
}
```


## Support

- **GitHub Issues**: Report bugs and request features
- **Email**: Contact the McGill NLP team
- **Documentation**: Visit our comprehensive docs site
- **Community**: Join our discussions on GitHub


## License

This project is licensed under the MIT License. See the LICENSE file for details.